{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":68699,"databundleVersionId":7659021,"sourceType":"competition"},{"sourceId":3972,"sourceType":"datasetVersion","datasetId":2363}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/orestasdulinskas/steel-plate-defect-prediction?scriptVersionId=187948080\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"![Steel Plates](https://www.dsstainlesssteel.com/wp-content/uploads/2018/05/Stainless-Steel-Plate-Sheet-1.jpg)","metadata":{}},{"cell_type":"markdown","source":"# Steel Plate Defect Prediction\n---\n## Background\nThe Steel Plates Faults dataset project stems from the need to enhance quality control measures in steel manufacturing processes. With the continuous demand for high-quality steel products across various industries, it becomes imperative to identify and rectify faults in steel plates efficiently. By leveraging machine learning and data science techniques, this project aims to develop predictive models capable of accurately classifying and detecting various types of faults present in steel plates. By doing so, manufacturers can streamline their quality assurance processes, reduce downtime, minimize production costs, and ultimately deliver superior-quality steel products to their customers. This project not only addresses immediate operational challenges but also lays the foundation for implementing proactive fault detection strategies to improve overall productivity and competitiveness in the steel industry.\n\n## Objective\nThe primary objective of this project is to develop robust predictive models capable of accurately predicting the presence and type of faults in steel plates. By utilizing machine learning algorithms, the project aims to predict the occurrence of various types of faults, including Pastry, Z_Scratch, K_Scatch, Stains, Dirtiness, Bumps, and Other_Faults, based on the features provided in the dataset. These predictive models will enable steel manufacturers to automate and enhance their quality control processes, allowing for timely identification and rectification of faults in steel plates during the manufacturing process.\n\n## Data\nThe Steel Plates Faults dataset comprises various features related to steel plates and different types of faults associated with them. The dataset includes the following features:\n\n* **id**: Unique identifier for each observation.\n* **X_Minimum**: Minimum x-coordinate of the defect.\n* **X_Maximum**: Maximum x-coordinate of the defect.\n* **Y_Minimum**: Minimum y-coordinate of the defect.\n* **Y_Maximum**: Maximum y-coordinate of the defect.\n* **Pixels_Areas**: Area of the defect in pixels.\n* **X_Perimeter**: Perimeter of the defect in the x-direction.\n* **Y_Perimeter**: Perimeter of the defect in the y-direction.\n* **Sum_of_Luminosity**: Sum of luminosity values within the defect area.\n* **Minimum_of_Luminosity**: Minimum luminosity value within the defect area.\n* **Maximum_of_Luminosity**: Maximum luminosity value within the defect area.\n* **Length_of_Conveyer**: Length of the conveyer belt during manufacturing.\n* **TypeOfSteel_A300**: Indicator variable for type of steel (A300).\n* **TypeOfSteel_A400**: Indicator variable for type of steel (A400).\n* **Steel_Plate_Thickness**: Thickness of the steel plate.\n* **Edges_Index**: Ratio of perimeter to the length of the defect.\n* **Empty_Index**: Ratio of empty pixels to the total number of pixels in the defect.\n* **Square_Index**: Ratio of area to the square of the perimeter of the defect.\n* **Outside_X_Index**: Ratio of pixels outside the defect in the x-direction to the total number of pixels in the defect.\n* **Edges_X_Index**: Ratio of horizontal edges to the total number of edges in the defect.\n* **Edges_Y_Index**: Ratio of vertical edges to the total number of edges in the defect.\n* **Outside_Global_Index**: Ratio of pixels outside the defect to the total number of pixels in the image.\n* **LogOfAreas**: Logarithm of the defect area.\n* **Log_X_Index**: Logarithm of the maximum length of the defect in the x-direction.\n* **Log_Y_Index**: Logarithm of the maximum length of the defect in the y-direction.\n* **Orientation_Index**: Index representing the orientation of the defect.\n* **Luminosity_Index**: Index representing the luminosity of the defect.\n* **SigmoidOfAreas**: Sigmoid function applied to the defect area.\n\n* **Pastry**: Binary indicator variable for the presence of the 'Pastry' fault.\n* **Z_Scratch**: Binary indicator variable for the presence of the 'Z_Scratch' fault.\n* **K_Scatch**: Binary indicator variable for the presence of the 'K_Scatch' fault.\n* **Stains**: Binary indicator variable for the presence of the 'Stains' fault.\n* **Dirtiness**: Binary indicator variable for the presence of the 'Dirtiness' fault.\n* **Bumps**: Binary indicator variable for the presence of the 'Bumps' fault.\n* **Other_Faults**: Binary indicator variable for the presence of other types of faults not specified above.\n\nThese features provide comprehensive information about the characteristics of steel plates and the types of faults they may exhibit, facilitating the development of predictive models for fault detection and classification.","metadata":{}},{"cell_type":"markdown","source":"# Data Cleaning\n---","metadata":{}},{"cell_type":"code","source":"# Import dataset\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\npath = r'/kaggle/input/playground-series-s4e3/train.csv'\ndf = pd.read_csv(path)\ndf.set_index('id', inplace=True)\n\ndf_org   = pd.read_csv('/kaggle/input/faulty-steel-plates/faults.csv')\ndf = pd.concat([df, df_org], ignore_index=True)\n\ndf.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-12T11:57:52.198976Z","iopub.execute_input":"2024-07-12T11:57:52.199398Z","iopub.status.idle":"2024-07-12T11:57:52.701022Z","shell.execute_reply.started":"2024-07-12T11:57:52.199362Z","shell.execute_reply":"2024-07-12T11:57:52.700009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nsns.set_theme()\n\ndf.info() # Summary of DataFrame information\n\nprint('\\nNumber of unique values in each column')\nfor i in df.columns:\n    print(f'{i} - {df[i].nunique()}')\n\nprint('\\nNumber of duplicated rows\\n', df.duplicated().sum())    \n    \nprint('\\nNumber of missing values in each column\\n', df.isnull().sum())\n\nsns.heatmap(df.isnull())","metadata":{"execution":{"iopub.status.busy":"2024-07-12T11:57:53.612432Z","iopub.execute_input":"2024-07-12T11:57:53.612835Z","iopub.status.idle":"2024-07-12T11:57:56.009356Z","shell.execute_reply.started":"2024-07-12T11:57:53.612802Z","shell.execute_reply":"2024-07-12T11:57:56.008203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2024-07-12T11:57:56.010891Z","iopub.execute_input":"2024-07-12T11:57:56.011222Z","iopub.status.idle":"2024-07-12T11:57:56.116054Z","shell.execute_reply.started":"2024-07-12T11:57:56.011195Z","shell.execute_reply":"2024-07-12T11:57:56.114945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Summary: Since I am using cleaned dataset, there are not missing or duplicate values","metadata":{}},{"cell_type":"markdown","source":"# Feature Selection\n---","metadata":{}},{"cell_type":"code","source":"# Defining training and target features\ntarget = ['Stains', 'Dirtiness', 'Z_Scratch', 'K_Scatch', 'Pastry', 'Bumps', 'Other_Faults']","metadata":{"execution":{"iopub.status.busy":"2024-07-12T11:57:57.355627Z","iopub.execute_input":"2024-07-12T11:57:57.35646Z","iopub.status.idle":"2024-07-12T11:57:57.361267Z","shell.execute_reply.started":"2024-07-12T11:57:57.356423Z","shell.execute_reply":"2024-07-12T11:57:57.359778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\n\npx.imshow(df.corr()[target].drop(target, axis=0).T, zmin=0, zmax=1, title='Correlation Matrix', template='ggplot2')","metadata":{"execution":{"iopub.status.busy":"2024-07-12T13:06:06.002907Z","iopub.execute_input":"2024-07-12T13:06:06.003339Z","iopub.status.idle":"2024-07-12T13:06:06.27991Z","shell.execute_reply.started":"2024-07-12T13:06:06.003307Z","shell.execute_reply":"2024-07-12T13:06:06.27877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = {}\nfor i in target:\n    features[i] = df.corr()[target].drop(target, axis=0)[i].sort_values().tail(3).keys().to_list()","metadata":{"execution":{"iopub.status.busy":"2024-07-12T11:57:58.804377Z","iopub.execute_input":"2024-07-12T11:57:58.804655Z","iopub.status.idle":"2024-07-12T11:57:59.301431Z","shell.execute_reply.started":"2024-07-12T11:57:58.804632Z","shell.execute_reply":"2024-07-12T11:57:59.300386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n# Train the model\nmodel = RandomForestClassifier()\nmodel.fit(df.drop(target, axis=1), df[target])\n\n# Get feature importances\nimportances = model.feature_importances_\n\n# Convert the importances into a DataFrame\nfeature_importance_df = pd.DataFrame({\n    'Feature': df.drop(target, axis=1).columns,\n    'Importance': importances\n})\n\n# Sort the DataFrame to plot\nfeature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n\npx.bar(feature_importance_df, x='Feature', y='Importance', template='ggplot2', title='Feature Importances')","metadata":{"execution":{"iopub.status.busy":"2024-07-12T11:58:00.043409Z","iopub.execute_input":"2024-07-12T11:58:00.043788Z","iopub.status.idle":"2024-07-12T11:58:19.411279Z","shell.execute_reply.started":"2024-07-12T11:58:00.043759Z","shell.execute_reply":"2024-07-12T11:58:19.410239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)\n---","metadata":{}},{"cell_type":"code","source":"subject = pd.DataFrame()\nfor i in target:\n    subject[i] = df[i].value_counts()\n    \npx.bar(subject.T, x=subject.T.index, y=subject.T.columns,\n       title='Label distribution', template='ggplot2',\n       labels={'values':'Count','index':'Target features', 'Stains':'Labels'},\n      text_auto=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T11:58:19.412788Z","iopub.execute_input":"2024-07-12T11:58:19.413103Z","iopub.status.idle":"2024-07-12T11:58:19.493049Z","shell.execute_reply.started":"2024-07-12T11:58:19.413077Z","shell.execute_reply":"2024-07-12T11:58:19.492084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Summary**:\n* **Low Incidence Faults**: Stains and Dirtiness are relatively rare.\n* **Moderate Faults**: Z_Scratch and Pastry faults occur more frequently.\n* **Significant Faults**: K_Scatch and Bumps are more common.\n* **Most Common Fault**: Other Faults category is the most prevalent.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfor j in features.keys():\n    print('\\n<<', j, '>>\\n')\n    for i in features[j]:\n        fig, ax = plt.subplots(figsize=(15, 4))\n        fig = sns.histplot(data=df, x=i, hue=j,bins=50, kde=True)\n        fig.set_title(f'{i} ')\n        fig.grid(True)\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-12T11:58:19.494288Z","iopub.execute_input":"2024-07-12T11:58:19.494589Z","iopub.status.idle":"2024-07-12T11:58:35.079337Z","shell.execute_reply.started":"2024-07-12T11:58:19.494564Z","shell.execute_reply":"2024-07-12T11:58:35.078289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from itertools import combinations\n\nfor t in target:\n    print('\\n<<', j, '>>\\n')\n    for i, j in combinations(features[t], 2):\n        plt.figure(figsize=(15, 6))\n        sns.scatterplot(data=df, x=i, y=j, hue=t, alpha=0.5, edgecolor=None)\n        plt.title(f\"{i} and {j}\")\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-12T11:58:35.081563Z","iopub.execute_input":"2024-07-12T11:58:35.081897Z","iopub.status.idle":"2024-07-12T11:59:00.048246Z","shell.execute_reply.started":"2024-07-12T11:58:35.081864Z","shell.execute_reply":"2024-07-12T11:59:00.046449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pre-Processing\n---","metadata":{}},{"cell_type":"code","source":"#df_copy = df.copy()","metadata":{"execution":{"iopub.status.busy":"2024-07-12T11:59:00.050132Z","iopub.execute_input":"2024-07-12T11:59:00.050513Z","iopub.status.idle":"2024-07-12T11:59:00.055101Z","shell.execute_reply.started":"2024-07-12T11:59:00.050483Z","shell.execute_reply":"2024-07-12T11:59:00.054079Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing Outliers","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"\"\"\"print('Lenght of data before removing outliers:', len(df))\nfor i in df.drop(target, axis=1).columns:\n    Q1 = df[i].quantile(0.15)\n    Q3 = df[i].quantile(0.85)\n    IQR = Q3 - Q1\n    df = df[(df[i] >= Q1 - 1.5*IQR) & (df[i] <= Q3 + 1.5*IQR)]\n    \nprint('Length of data after removing outliers:', len(df))\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-07-12T11:59:00.056341Z","iopub.execute_input":"2024-07-12T11:59:00.056689Z","iopub.status.idle":"2024-07-12T11:59:00.069413Z","shell.execute_reply.started":"2024-07-12T11:59:00.056663Z","shell.execute_reply":"2024-07-12T11:59:00.068291Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Engineering","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\ndef feature_engineering(df):\n    epsilon = 1e-6\n    \n    # Calculate area\n    df['X_Distance'] = df['X_Maximum'] - df['X_Minimum']\n    df['Y_Distance'] = df['Y_Maximum'] - df['Y_Minimum']\n    df['Area'] = (df['X_Distance']) * (df['Y_Distance'])\n    \n    # Density Feature\n    #df['Density'] = df['Pixels_Areas'] / (df['X_Perimeter'] + df['Y_Perimeter'])\n\n    # Calculate perimeter\n    df['Perimeter'] = 2 * ((df['X_Maximum'] - df['X_Minimum']) + (df['Y_Maximum'] - df['Y_Minimum']))\n    \n    # Relative Perimeter Feature\n    df['Relative_Perimeter'] = df['X_Perimeter'] / (df['X_Perimeter'] + df['Y_Perimeter'] + epsilon)\n    \n    # Circularity Feature\n    #df['Circularity'] = df['Pixels_Areas'] / (df['X_Perimeter'] ** 2)\n    \n    # Combined Geometric Index Feature\n    df['Combined_Geometric_Index'] = df['Edges_Index'] * df['Square_Index']\n    \n    # Symmetry Index Feature\n    df['Symmetry_Index'] = np.abs(df['X_Distance'] - df['Y_Distance']) / (df['X_Distance'] + df['Y_Distance'] + epsilon)\n    \n    # Compute mean, median, and standard deviation\n    df['Mean_Luminosity'] = df[['Sum_of_Luminosity', 'Minimum_of_Luminosity', 'Maximum_of_Luminosity']].mean(axis=1)\n    df['Median_Luminosity'] = df[['Sum_of_Luminosity', 'Minimum_of_Luminosity', 'Maximum_of_Luminosity']].median(axis=1)\n    df['Std_Luminosity'] = df[['Sum_of_Luminosity', 'Minimum_of_Luminosity', 'Maximum_of_Luminosity']].std(axis=1)\n    \n    # Calculate aspect ratio\n    df['Aspect_Ratio'] = (df['Y_Maximum'] - df['Y_Minimum']) / (df['X_Maximum'] - df['X_Minimum'])\n    \n    # Apply logarithmic transformation\n    df['Log_Pixels_Areas'] = np.log(df['Pixels_Areas'])\n    \n    # Interaction Term Feature\n    df['X_Distance*Pixels_Areas'] = df['X_Distance'] * df['Pixels_Areas']\n    \n    # Create composite feature\n    df['Luminosity_Index_Product'] = df['Luminosity_Index'] * df['Sum_of_Luminosity']\n    \n    # Color Contrast Feature\n    df['Color_Contrast'] = df['Maximum_of_Luminosity'] - df['Minimum_of_Luminosity']\n    \n    # Average Luminosity Feature\n    df['Average_Luminosity'] = (df['Sum_of_Luminosity'] + df['Minimum_of_Luminosity']) / 2\n    \n    # Generate interaction feature\n    df['Area_Pixels_Interact'] = df['Area'] * df['Pixels_Areas']\n    \n    # Additional Features\n    df['sin_orientation'] = np.sin(df['Orientation_Index'])\n    df['Edges_Index2'] = np.exp(df['Edges_Index'] + epsilon)\n    df['X_Maximum2'] = np.sin(df['X_Maximum'])\n    df['Y_Minimum2'] = np.sin(df['Y_Minimum'])\n    df['Aspect_Ratio_Pixels'] = np.where(df['Y_Perimeter'] == 0, 0, df['X_Perimeter'] / df['Y_Perimeter'])\n    df['Aspect_Ratio'] = np.where(df['Y_Distance'] == 0, 0, df['X_Distance'] / df['Y_Distance'])\n\n    # Normalized Steel Thickness Feature\n    df['Normalized_Steel_Thickness'] = (df['Steel_Plate_Thickness'] - df['Steel_Plate_Thickness'].min()) / (df['Steel_Plate_Thickness'].max() - df['Steel_Plate_Thickness'].min())\n\n    # Logarithmic Features\n    df['Log_Perimeter'] = np.log(df['X_Perimeter'] + df['Y_Perimeter'] + epsilon)\n    df['Log_Luminosity'] = np.log(df['Sum_of_Luminosity'] + epsilon)\n    df['Log_Aspect_Ratio'] = np.log(df['Aspect_Ratio'] ** 2 + epsilon)\n\n    # Statistical Features\n    df['Combined_Index'] = df['Orientation_Index'] * df['Luminosity_Index']\n    df['Sigmoid_Areas'] = 1 / (1 + np.exp(-df['LogOfAreas'] + epsilon))\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2024-07-12T11:59:00.070816Z","iopub.execute_input":"2024-07-12T11:59:00.071178Z","iopub.status.idle":"2024-07-12T11:59:00.08688Z","shell.execute_reply.started":"2024-07-12T11:59:00.071134Z","shell.execute_reply":"2024-07-12T11:59:00.085804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Applying feature engineering to the dataframe\ndf = feature_engineering(df)\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-12T11:59:00.088251Z","iopub.execute_input":"2024-07-12T11:59:00.08857Z","iopub.status.idle":"2024-07-12T11:59:00.146203Z","shell.execute_reply.started":"2024-07-12T11:59:00.088544Z","shell.execute_reply":"2024-07-12T11:59:00.145191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Scaling","metadata":{}},{"cell_type":"code","source":"#df = df_copy.copy()","metadata":{"execution":{"iopub.status.busy":"2024-07-12T11:59:00.14738Z","iopub.execute_input":"2024-07-12T11:59:00.147692Z","iopub.status.idle":"2024-07-12T11:59:00.151841Z","shell.execute_reply.started":"2024-07-12T11:59:00.147667Z","shell.execute_reply":"2024-07-12T11:59:00.150758Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, Normalizer\n\n\"\"\"scaler = StandardScaler()\ndf[df.drop(target, axis=1).columns] = scaler.fit_transform(df.drop(target, axis=1))\"\"\"\n\nscaler = MinMaxScaler()\ndf[df.drop(target, axis=1).columns] = scaler.fit_transform(df.drop(target, axis=1))\n\n\"\"\"scaler = MaxAbsScaler()\ndf[df.drop(target, axis=1).columns] = scaler.fit_transform(df.drop(target, axis=1))\"\"\"\n\n\"\"\"scaler = Normalizer()\ndf[df.drop(target, axis=1).columns] = scaler.fit_transform(df.drop(target, axis=1))\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-07-12T11:59:00.156942Z","iopub.execute_input":"2024-07-12T11:59:00.157344Z","iopub.status.idle":"2024-07-12T11:59:00.201293Z","shell.execute_reply.started":"2024-07-12T11:59:00.157313Z","shell.execute_reply":"2024-07-12T11:59:00.200249Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Selection","metadata":{}},{"cell_type":"code","source":"\"\"\"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import hamming_loss\n\nX = df[df.drop(target, axis=1).columns[selector.support_]]\ny = df[target]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# Initialize the RandomForestClassifier\nrf_classifier = RandomForestClassifier()\n\n# Fit the model\nrf_classifier.fit(X_train, y_train)\n\n# Predict labels\ny_pred = rf_classifier.predict(X_test)\n\n# Ensure y_test is a DataFrame or 2D array\nif isinstance(y_test, pd.Series):\n    y_test = y_test.to_frame()\n\n# Calculate Hamming Loss\nhamming = hamming_loss(y_test, y_pred)\nprint(f\"Hamming Loss: {hamming}\")\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-07-12T11:59:00.202644Z","iopub.execute_input":"2024-07-12T11:59:00.202969Z","iopub.status.idle":"2024-07-12T11:59:00.209546Z","shell.execute_reply.started":"2024-07-12T11:59:00.202942Z","shell.execute_reply":"2024-07-12T11:59:00.208508Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#comp = pd.DataFrame(columns=['Hamming Loss'])","metadata":{"execution":{"iopub.status.busy":"2024-07-12T11:59:00.210899Z","iopub.execute_input":"2024-07-12T11:59:00.211346Z","iopub.status.idle":"2024-07-12T11:59:00.220631Z","shell.execute_reply.started":"2024-07-12T11:59:00.21131Z","shell.execute_reply":"2024-07-12T11:59:00.21958Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#comp.loc['minmax rescaler + feat engineer + feat select'] = hamming\n#comp","metadata":{"execution":{"iopub.status.busy":"2024-07-12T11:59:00.221977Z","iopub.execute_input":"2024-07-12T11:59:00.222338Z","iopub.status.idle":"2024-07-12T11:59:00.231316Z","shell.execute_reply.started":"2024-07-12T11:59:00.222304Z","shell.execute_reply":"2024-07-12T11:59:00.230365Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Results of comparing different pre-processing methods\n* base\t0.105286\n* removed 5% outliers\t0.105719\n* removed 10% outliers\t0.107542\n* removed 15% outliers\t0.122124\n* standard rescaler\t0.103902\n* minmax rescaler\t0.103666\n* maxabs rescaler\t0.104206\n* minmax rescaler + feat engineer\t0.104240\n* minmax rescaler + feat engineer + feat select\t0.108797","metadata":{}},{"cell_type":"code","source":"\"\"\"from sklearn.feature_selection import RFECV\n\nX = df.drop(target, axis=1)\ny = df[target]\n\nestimator = RandomForestClassifier()\n\n# Initialize RFE with cross-validation\nselector = RFECV(estimator, step=1, cv=5)\n\n# Fit RFE to the data\nselector.fit(X, y)\n\n# Explore the results (e.g., selected features, accuracy)\nprint(\"Number of selected features:\", selector.n_features_)\nprint(\"Selected features indices:\", selector.support_)\nprint(\"Ranking of features:\", selector.ranking_)\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-07-12T11:59:00.232607Z","iopub.execute_input":"2024-07-12T11:59:00.232896Z","iopub.status.idle":"2024-07-12T11:59:00.245617Z","shell.execute_reply.started":"2024-07-12T11:59:00.232873Z","shell.execute_reply":"2024-07-12T11:59:00.244363Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"cv_results = pd.DataFrame(selector.cv_results_)\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Mean test accuracy\")\nplt.errorbar(\n    x=cv_results.index,\n    y=cv_results[\"mean_test_score\"],\n    yerr=cv_results[\"std_test_score\"],\n)\nplt.title(\"Recursive Feature Elimination \\nwith correlated features\")\nplt.show()\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-07-12T11:59:00.246958Z","iopub.execute_input":"2024-07-12T11:59:00.247345Z","iopub.status.idle":"2024-07-12T11:59:00.260406Z","shell.execute_reply.started":"2024-07-12T11:59:00.247316Z","shell.execute_reply":"2024-07-12T11:59:00.259318Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = ['X_Minimum',\n 'X_Maximum',\n 'Y_Maximum',\n 'Pixels_Areas',\n 'Length_of_Conveyer',\n 'Steel_Plate_Thickness',\n 'Empty_Index',\n 'Outside_X_Index',\n 'Log_X_Index',\n 'Orientation_Index',\n 'Luminosity_Index',\n 'X_Distance',\n 'Combined_Geometric_Index',\n 'Std_Luminosity',\n 'X_Distance*Pixels_Areas',\n 'Luminosity_Index_Product',\n 'Color_Contrast',\n 'Combined_Index',\n 'Sigmoid_Areas']","metadata":{"execution":{"iopub.status.busy":"2024-07-12T11:59:00.261917Z","iopub.execute_input":"2024-07-12T11:59:00.262775Z","iopub.status.idle":"2024-07-12T11:59:00.274607Z","shell.execute_reply.started":"2024-07-12T11:59:00.262735Z","shell.execute_reply":"2024-07-12T11:59:00.273565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Training\n---","metadata":{}},{"cell_type":"code","source":"def compare_models(model_scores):\n    subejct = pd.DataFrame.from_dict(model_scores, orient='index',columns=['Score'])\n    fig = px.bar(subejct, x=subejct.index, y='Score', template='ggplot2', text_auto=True,\n                 range_y=[0, 100], labels={'index':'Models'})\n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-12T11:59:00.275854Z","iopub.execute_input":"2024-07-12T11:59:00.276442Z","iopub.status.idle":"2024-07-12T11:59:00.285317Z","shell.execute_reply.started":"2024-07-12T11:59:00.276412Z","shell.execute_reply":"2024-07-12T11:59:00.284229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom xgboost import XGBClassifier\n\nclassifiers = {\n    \"Logisitic Regression\": {'model':LogisticRegression()},\n    \n    \"Weighted Logisitic Regression\": {'model':LogisticRegression(class_weight='balanced')},\n        \n    \"Decision Tree Classifier\": {'model':DecisionTreeClassifier()},\n    \n    \"Random Forest Classifier\": {'model':RandomForestClassifier()},\n    \n    \"Extra Trees Classifier\": {'model':ExtraTreesClassifier()},\n    \n    \"Naive Bayes\": {'model':GaussianNB()},\n    \n    \"Voting Classifier\": {'model':VotingClassifier(estimators=[\n                        ('lr', LogisticRegression()),\n                        ('rf', RandomForestClassifier()),\n                        ('gnb', GaussianNB())\n                    ], voting='soft')},\n    \n    \"Gradient Boosting Classifier\": {'model':GradientBoostingClassifier()},\n    \n    \"XGBoost Classifier\": {'model':XGBClassifier(objective='binary:logistic')}\n}","metadata":{"execution":{"iopub.status.busy":"2024-07-12T11:59:00.286815Z","iopub.execute_input":"2024-07-12T11:59:00.28772Z","iopub.status.idle":"2024-07-12T11:59:00.322503Z","shell.execute_reply.started":"2024-07-12T11:59:00.287681Z","shell.execute_reply":"2024-07-12T11:59:00.321461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score\n\nbest_models = {}\n\nmodel_scores = {}\n\n# Iterating through each target\nfor t in target:\n    \n    print('\\n<<' ,t, '>>\\n')\n    \n    model_scores = {}\n    \n    X = df.drop(target, axis=1)[features]\n    y = df[t]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n    # Iterating through models\n    for key, classifier in classifiers.items():\n\n        # Fitting the model\n        try:\n            classifier['model'].fit(X_train, y_train, eval_metric='auc')\n        except TypeError:\n            classifier['model'''].fit(X_train, y_train)\n\n        # Evaluating model performance\n        pred = classifier['model'].predict_proba(X_test)\n        pred = [proba[1] for proba in pred]\n        pred = np.array(pred)\n        training_score = roc_auc_score(y_test, pred)\n        model_scores[key] = round(training_score.mean() * 100, 2)\n\n    # Saving best performing model for current label\n    best_models[t] = [item[0] for item in sorted(model_scores.items(), key=lambda item: item[1], reverse=True)[:1]]\n\n    # Comparing model performance for current label\n    compare_models(model_scores)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T11:59:00.323935Z","iopub.execute_input":"2024-07-12T11:59:00.324417Z","iopub.status.idle":"2024-07-12T12:01:30.371657Z","shell.execute_reply.started":"2024-07-12T11:59:00.324379Z","shell.execute_reply":"2024-07-12T12:01:30.37056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Optimization\n---","metadata":{}},{"cell_type":"code","source":"# Defining choose model function that sets up model for training\ndef choose_model(params):\n    match best_models[t][0]:\n        case \"Logisitic Regression\":\n            model = LogisticRegression(**params)\n            \n        case \"Weighted Logisitic Regression\":\n            model = LogisticRegression(**params, class_weight='balanced')\n\n        case \"Decision Tree Classifier\":\n            model = DecisionTreeClassifier(**params)\n\n        case \"Random Forest Classifier\":\n            model = RandomForestClassifier(**params)\n            \n        case \"Extra Trees Classifier\":\n            model = ExtraTreesClassifier(**params)\n            \n        case \"Naive Bayes\":\n            model = GaussianNB(**params)\n            \n        case \"Voting Classifier\":\n            model = VotingClassifier(estimators=[\n                        ('lr', LogisticRegression()),\n                        ('rf', RandomForestClassifier()),\n                        ('gnb', GaussianNB())\n                    ], voting='soft', **params)\n\n        case \"Gradient Boosting Classifier\":\n            model = GradientBoostingClassifier(**params)\n\n        case \"XGBoost Classifier\":\n            model = XGBClassifier(objective='binary:logistic',**params)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-07-12T12:01:30.373386Z","iopub.execute_input":"2024-07-12T12:01:30.373799Z","iopub.status.idle":"2024-07-12T12:01:30.381869Z","shell.execute_reply.started":"2024-07-12T12:01:30.373764Z","shell.execute_reply":"2024-07-12T12:01:30.380703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining objective function for Optuna optimization\ndef objective(trial):\n    \n    # Specifying hyperparameters\n\n    classifiers = {\n        \"Logisitic Regression\": {'model':LogisticRegression(), 'params':{\n            'C': trial.suggest_float('C', 0.01, 10.0, log=True),\n            'penalty': trial.suggest_categorical('penalty', ['l1', 'l2']),\n            'solver': trial.suggest_categorical('solver', ['liblinear', 'saga']),\n        }},\n        \n        \"Weighted Logisitic Regression\": {'model':LogisticRegression(), 'params':{\n            'class_weight': trial.suggest_categorical('class_weight', ['balanced', None]),\n            'C': trial.suggest_float('C', 0.01, 10.0, log=True),\n            'penalty': trial.suggest_categorical('penalty', ['l1', 'l2']),\n            'solver': trial.suggest_categorical('solver', ['liblinear', 'saga']),\n        }},\n        \n        \"Decision Tree Classifier\": {\n        'model': DecisionTreeClassifier(),\n        'params': {\n            'max_depth': trial.suggest_int('max_depth', 1, 100, log=False, step=1),\n            'min_samples_split': trial.suggest_int('min_samples_split', 2, 20, log=False, step=1),\n            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20, log=False, step=1)\n            }\n        },\n        \n        \"Random Forest Classifier\": {\n            'model': RandomForestClassifier(),\n            'params': {\n                'min_samples_split': trial.suggest_int('min_samples_split', 2, 20, log=False, step=1),\n                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20, log=False, step=1),\n                'max_depth': trial.suggest_int('max_depth', 1, 100, log=False, step=1),\n                'max_features': trial.suggest_categorical('max_features', [1, 'sqrt', 'log2', None]),\n                'n_estimators': trial.suggest_int('n_estimators', 100, 2000)\n            }\n        },\n        \n        \"Extra Trees Classifier\": {\n            'model': ExtraTreesClassifier(),\n            'params': {\n                'n_estimators': trial.suggest_int('n_estimators', 100, 2000),\n                'max_depth': trial.suggest_int('max_depth', 1, 100, log=False, step=1),\n                'min_samples_split': trial.suggest_int('min_samples_split', 2, 20, log=False, step=1),\n                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20, log=False, step=1),\n                'max_features': trial.suggest_categorical('max_features', [1, 'sqrt', 'log2', None]),\n            }\n        },\n        \n        \"Naive Bayes\": {\n            'model': GaussianNB(),\n            'params': {\n                'var_smoothing': trial.suggest_float('var_smoothing', 1e-9, 1e-3, log=True),\n            }\n        },\n        \n        \"Voting Classifier\": {\n            'model': VotingClassifier(estimators=[\n                        ('lr', LogisticRegression()),\n                        ('rf', RandomForestClassifier()),\n                        ('gnb', GaussianNB())\n                    ], voting='soft'),\n            'params': {\n                'weights': trial.suggest_categorical('weights', ['1,1,1', '2,1,1', '1,2,1']),\n            }\n        },\n        \n        \"Gradient Boosting Classifier\": {\n            'model': GradientBoostingClassifier(),\n            'params': {\n                'n_estimators': trial.suggest_int('n_estimators', 100, 2000),\n                'learning_rate': trial.suggest_float('learning_rate', 0.001, 1.0, log=False),\n                'max_depth': trial.suggest_int('max_depth', 1, 100, log=False, step=1),\n                'subsample': trial.suggest_float('subsample', 0.1, 1),\n            }\n        },\n        \"XGBoost Classifier\": {\n            'model': XGBClassifier(objective='binary:logistic'),\n            'params': {\n                'learning_rate': trial.suggest_float('learning_rate', 0.001, 1.0, log=False),\n                'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),\n                'gamma': trial.suggest_float('gamma', 0, 1),\n                'subsample': trial.suggest_float('subsample', 0.1, 1),\n                'max_depth': trial.suggest_int('max_depth', 1, 100, log=False, step=1),\n                'n_estimators': trial.suggest_int('n_estimators', 100, 2000),\n                \"booster\": \"gbtree\",\n                \"reg_alpha\": trial.suggest_float('reg_alpha', 0.1, 1),\n                \"reg_lambda\": trial.suggest_float('reg_lambda', 0, 1),\n                \"colsample_bytree\": trial.suggest_float('colsample_bytree', 0.1, 1)\n            }\n        }\n    }\n    \n    # Selecting best performing model for current label\n    model = choose_model(classifiers[best_models[t][0]]['params'])\n\n    model.fit(X_train, y_train) # Fitting training data to the model\n    \n    pred = model.predict_proba(X_test)\n    pred = [proba[1] for proba in pred]\n    pred = np.array(pred)\n    score = roc_auc_score(y_test, pred) # Evaluating model perfomance using ROC-AUC score\n    \n    return score","metadata":{"execution":{"iopub.status.busy":"2024-07-12T12:01:30.383779Z","iopub.execute_input":"2024-07-12T12:01:30.384177Z","iopub.status.idle":"2024-07-12T12:01:30.40485Z","shell.execute_reply.started":"2024-07-12T12:01:30.384133Z","shell.execute_reply":"2024-07-12T12:01:30.403641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import optuna\nimport plotly\n\nmodel_grid_scores = {} # Saving models scores\nbest_grid_models = {} # Saving best performing models\ntest_datasets = {} # Saving test datasets specific to target trained for later evaluations\n\n# Iterating through targets\nfor t in target:\n    \n    print('\\n<<' ,t, '>>\\n')\n    \n    X = df.drop(target, axis=1)[features]\n    y = df[t]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n    \n    test_datasets[t] = {'X':X_test, 'y':y_test}\n\n    # Initializing Optuna study\n    study = optuna.create_study(direction='maximize')\n\n    # Performing Hyperparameter optimization using optuna objective function\n    print('Training', best_models[t][0], '\\n')\n    study.optimize(objective, n_trials=10) # Number of trials\n\n    print('Best trial parameters:', study.best_trial.params)\n    print('\\n Best ROC-AUC score:', study.best_trial.value)\n\n    # Selecting best hyperparameter combination\n    best_trial = study.best_trial # Getting best trial\n    best_params = best_trial.params # Getting best trial parameters\n    parameters = set(classifiers[best_models[t][0]]['model'].get_params().keys()) & set(best_params.keys()) # Getting model parameter keys\n    best_params = {key: best_params[key] for key in parameters} # Choosing parameters appropriate for selected model\n    best = choose_model(best_params) # Choosing the model and assigning its parameters\n    best.fit(X_train, y_train) # Fitting the model\n\n    best_grid_models[t] = best # Saving the model in a dictionary\n\n    # Evaluating model performance\n    pred = best.predict_proba(X_test)\n    pred = [proba[1] for proba in pred]\n    pred = np.array(pred)\n    score = roc_auc_score(y_test, pred)\n    model_grid_scores[t] = round(score.mean(), 2) * 100\n\n    optuna.visualization.plot_optimization_history(study).show() # Visualising Optimization history\n    optuna.visualization.plot_param_importances(study).show() # Visualising Parameter importances","metadata":{"execution":{"iopub.status.busy":"2024-07-12T12:01:30.406446Z","iopub.execute_input":"2024-07-12T12:01:30.407103Z","iopub.status.idle":"2024-07-12T12:40:15.003862Z","shell.execute_reply.started":"2024-07-12T12:01:30.407065Z","shell.execute_reply":"2024-07-12T12:40:15.002731Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluating model performances for each label\ncompare_models(model_grid_scores)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Evaluation\n---","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\n\ndef performance_metrics(model):\n    \n    print('\\n<<' ,t, '>>\\n')\n    \n    preds = model.predict(test_datasets[t]['X'])\n    \n    print(classification_report(test_datasets[t]['y'], preds), '\\n')\n\n    cf_matrix = confusion_matrix(test_datasets[t]['y'], preds, normalize='all')\n    fig = px.imshow(pd.DataFrame(cf_matrix), \n          template='ggplot2', title='Confusion Matrix', aspect='auto', text_auto=True, zmin=0,zmax=1)\n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-12T12:40:15.064566Z","iopub.execute_input":"2024-07-12T12:40:15.064879Z","iopub.status.idle":"2024-07-12T12:40:15.071534Z","shell.execute_reply.started":"2024-07-12T12:40:15.064852Z","shell.execute_reply":"2024-07-12T12:40:15.070438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for t in target:\n    performance_metrics(best_grid_models[t])","metadata":{"execution":{"iopub.status.busy":"2024-07-12T12:40:15.073083Z","iopub.execute_input":"2024-07-12T12:40:15.073483Z","iopub.status.idle":"2024-07-12T12:40:18.403818Z","shell.execute_reply.started":"2024-07-12T12:40:15.073455Z","shell.execute_reply":"2024-07-12T12:40:18.402856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission\n---","metadata":{}},{"cell_type":"code","source":"# Importing test data\npath = r'/kaggle/input/playground-series-s4e3/test.csv'\ndf_test = pd.read_csv(path)\ndf_test.set_index('id', inplace=True)\nid = df_test.index\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-12T12:40:18.405097Z","iopub.execute_input":"2024-07-12T12:40:18.405426Z","iopub.status.idle":"2024-07-12T12:40:18.474523Z","shell.execute_reply.started":"2024-07-12T12:40:18.4054Z","shell.execute_reply":"2024-07-12T12:40:18.47345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Applying feature engineering\ndf_test = feature_engineering(df_test)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T12:40:18.475633Z","iopub.execute_input":"2024-07-12T12:40:18.475913Z","iopub.status.idle":"2024-07-12T12:40:18.516291Z","shell.execute_reply.started":"2024-07-12T12:40:18.47589Z","shell.execute_reply":"2024-07-12T12:40:18.515239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Keeping selected features\ndf_test = df_test[features]","metadata":{"execution":{"iopub.status.busy":"2024-07-12T12:40:18.521985Z","iopub.execute_input":"2024-07-12T12:40:18.522601Z","iopub.status.idle":"2024-07-12T12:40:18.528285Z","shell.execute_reply.started":"2024-07-12T12:40:18.522571Z","shell.execute_reply":"2024-07-12T12:40:18.527343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scaling data\ndf_test = scaler.fit_transform(df_test)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T12:40:18.529455Z","iopub.execute_input":"2024-07-12T12:40:18.52983Z","iopub.status.idle":"2024-07-12T12:40:18.545131Z","shell.execute_reply.started":"2024-07-12T12:40:18.529798Z","shell.execute_reply":"2024-07-12T12:40:18.544024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating submission dataframe with the index same as test data\nsubmission = pd.DataFrame(index=id)\n\n# Iterating through dataframes of each label\nfor t in target:\n    \n    print('\\n<<', t, '>>\\n')\n\n    X = df.drop(target, axis=1)[features]\n    y = df[t]\n    \n    # Training model on full data\n    model = best_grid_models[t]\n    model.fit(X, y)\n    \n    # Predicting probability\n    results = model.predict_proba(df_test)\n    results = [proba[1] for proba in results]\n    results = np.array(results)\n    submission[j] = results.T","metadata":{"execution":{"iopub.status.busy":"2024-07-12T12:40:18.546673Z","iopub.execute_input":"2024-07-12T12:40:18.547067Z","iopub.status.idle":"2024-07-12T12:54:25.958746Z","shell.execute_reply.started":"2024-07-12T12:40:18.547031Z","shell.execute_reply":"2024-07-12T12:54:25.957744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-12T12:54:25.959937Z","iopub.execute_input":"2024-07-12T12:54:25.960267Z","iopub.status.idle":"2024-07-12T12:54:25.970028Z","shell.execute_reply.started":"2024-07-12T12:54:25.960234Z","shell.execute_reply":"2024-07-12T12:54:25.968808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv')","metadata":{"execution":{"iopub.status.busy":"2024-07-12T12:54:26.051246Z","iopub.execute_input":"2024-07-12T12:54:26.05156Z","iopub.status.idle":"2024-07-12T12:54:26.086856Z","shell.execute_reply.started":"2024-07-12T12:54:26.051534Z","shell.execute_reply":"2024-07-12T12:54:26.085711Z"},"trusted":true},"execution_count":null,"outputs":[]}]}