{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":68699,"databundleVersionId":7659021,"sourceType":"competition"},{"sourceId":3972,"sourceType":"datasetVersion","datasetId":2363}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![Steel Plates](https://www.dsstainlesssteel.com/wp-content/uploads/2018/05/Stainless-Steel-Plate-Sheet-1.jpg)","metadata":{}},{"cell_type":"markdown","source":"# Steel Plate Defect Prediction\n---\n## Background\nThe Steel Plates Faults dataset project stems from the need to enhance quality control measures in steel manufacturing processes. With the continuous demand for high-quality steel products across various industries, it becomes imperative to identify and rectify faults in steel plates efficiently. By leveraging machine learning and data science techniques, this project aims to develop predictive models capable of accurately classifying and detecting various types of faults present in steel plates. By doing so, manufacturers can streamline their quality assurance processes, reduce downtime, minimize production costs, and ultimately deliver superior-quality steel products to their customers. This project not only addresses immediate operational challenges but also lays the foundation for implementing proactive fault detection strategies to improve overall productivity and competitiveness in the steel industry.\n\n## Objective\nThe primary objective of this project is to develop robust predictive models capable of accurately predicting the presence and type of faults in steel plates. By utilizing machine learning algorithms, the project aims to predict the occurrence of various types of faults, including Pastry, Z_Scratch, K_Scatch, Stains, Dirtiness, Bumps, and Other_Faults, based on the features provided in the dataset. These predictive models will enable steel manufacturers to automate and enhance their quality control processes, allowing for timely identification and rectification of faults in steel plates during the manufacturing process.\n\n## Data\nThe Steel Plates Faults dataset comprises various features related to steel plates and different types of faults associated with them. The dataset includes the following features:\n\n* **id**: Unique identifier for each observation.\n* **X_Minimum**: Minimum x-coordinate of the defect.\n* **X_Maximum**: Maximum x-coordinate of the defect.\n* **Y_Minimum**: Minimum y-coordinate of the defect.\n* **Y_Maximum**: Maximum y-coordinate of the defect.\n* **Pixels_Areas**: Area of the defect in pixels.\n* **X_Perimeter**: Perimeter of the defect in the x-direction.\n* **Y_Perimeter**: Perimeter of the defect in the y-direction.\n* **Sum_of_Luminosity**: Sum of luminosity values within the defect area.\n* **Minimum_of_Luminosity**: Minimum luminosity value within the defect area.\n* **Maximum_of_Luminosity**: Maximum luminosity value within the defect area.\n* **Length_of_Conveyer**: Length of the conveyer belt during manufacturing.\n* **TypeOfSteel_A300**: Indicator variable for type of steel (A300).\n* **TypeOfSteel_A400**: Indicator variable for type of steel (A400).\n* **Steel_Plate_Thickness**: Thickness of the steel plate.\n* **Edges_Index**: Ratio of perimeter to the length of the defect.\n* **Empty_Index**: Ratio of empty pixels to the total number of pixels in the defect.\n* **Square_Index**: Ratio of area to the square of the perimeter of the defect.\n* **Outside_X_Index**: Ratio of pixels outside the defect in the x-direction to the total number of pixels in the defect.\n* **Edges_X_Index**: Ratio of horizontal edges to the total number of edges in the defect.\n* **Edges_Y_Index**: Ratio of vertical edges to the total number of edges in the defect.\n* **Outside_Global_Index**: Ratio of pixels outside the defect to the total number of pixels in the image.\n* **LogOfAreas**: Logarithm of the defect area.\n* **Log_X_Index**: Logarithm of the maximum length of the defect in the x-direction.\n* **Log_Y_Index**: Logarithm of the maximum length of the defect in the y-direction.\n* **Orientation_Index**: Index representing the orientation of the defect.\n* **Luminosity_Index**: Index representing the luminosity of the defect.\n* **SigmoidOfAreas**: Sigmoid function applied to the defect area.\n\n* **Pastry**: Binary indicator variable for the presence of the 'Pastry' fault.\n* **Z_Scratch**: Binary indicator variable for the presence of the 'Z_Scratch' fault.\n* **K_Scatch**: Binary indicator variable for the presence of the 'K_Scatch' fault.\n* **Stains**: Binary indicator variable for the presence of the 'Stains' fault.\n* **Dirtiness**: Binary indicator variable for the presence of the 'Dirtiness' fault.\n* **Bumps**: Binary indicator variable for the presence of the 'Bumps' fault.\n* **Other_Faults**: Binary indicator variable for the presence of other types of faults not specified above.\n\nThese features provide comprehensive information about the characteristics of steel plates and the types of faults they may exhibit, facilitating the development of predictive models for fault detection and classification.","metadata":{}},{"cell_type":"markdown","source":"# Data Cleaning\n---","metadata":{}},{"cell_type":"code","source":"# Import dataset\nimport pandas as pd\n\npath = r'/kaggle/input/playground-series-s4e3/train.csv'\ndf = pd.read_csv(path)\ndf.set_index('id', inplace=True)\n\ndf_org   = pd.read_csv('/kaggle/input/faulty-steel-plates/faults.csv')\ndf = pd.concat([df, df_org], ignore_index=True)\n\ndf.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-02T09:57:20.037382Z","iopub.execute_input":"2024-04-02T09:57:20.037894Z","iopub.status.idle":"2024-04-02T09:57:20.828356Z","shell.execute_reply.started":"2024-04-02T09:57:20.037848Z","shell.execute_reply":"2024-04-02T09:57:20.826777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info() # Summary of DataFrame information\n\nprint('\\nNumber of unique values in each column')\nfor i in df.columns:\n    print(f'{i} - {df[i].nunique()}')\n\nprint('\\nNumber of missing values in each column\\n', df.isnull().sum())\n\nprint('\\nNumber of duplicated rows\\n', df.duplicated().sum())","metadata":{"execution":{"iopub.status.busy":"2024-04-02T09:57:20.830966Z","iopub.execute_input":"2024-04-02T09:57:20.831498Z","iopub.status.idle":"2024-04-02T09:57:20.904280Z","shell.execute_reply.started":"2024-04-02T09:57:20.831451Z","shell.execute_reply":"2024-04-02T09:57:20.902927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T09:57:20.906642Z","iopub.execute_input":"2024-04-02T09:57:20.907010Z","iopub.status.idle":"2024-04-02T09:57:21.054238Z","shell.execute_reply.started":"2024-04-02T09:57:20.906979Z","shell.execute_reply":"2024-04-02T09:57:21.052835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Summary: Since I am using cleaned dataset, there are not missing or duplicate values","metadata":{}},{"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)\n---","metadata":{}},{"cell_type":"code","source":"# Defining training and target features\ntarget = ['Stains', 'Dirtiness', 'Z_Scratch', 'K_Scatch', 'Pastry', 'Bumps', 'Other_Faults']\nfeatures = df.drop(target, axis=1).columns","metadata":{"execution":{"iopub.status.busy":"2024-04-02T09:57:21.056957Z","iopub.execute_input":"2024-04-02T09:57:21.057312Z","iopub.status.idle":"2024-04-02T09:57:21.065206Z","shell.execute_reply.started":"2024-04-02T09:57:21.057283Z","shell.execute_reply":"2024-04-02T09:57:21.063612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nplt.style.use('seaborn-v0_8-bright')\ncolors = sns.color_palette('bright')\nwarnings.filterwarnings(\"ignore\")\n\nfig, axes = plt.subplots(1, 7, figsize=(20, 3))  # 1 row, 7 columns\n\n# Plot pie charts for each target label\nfor i, colname in enumerate(target):\n    axes[i].pie(df[colname].value_counts(), labels=df[colname].unique(), autopct='%1.1f%%')\n    axes[i].set_title(colname)\n\n# Display the plot\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T09:57:21.066725Z","iopub.execute_input":"2024-04-02T09:57:21.067381Z","iopub.status.idle":"2024-04-02T09:57:23.466827Z","shell.execute_reply.started":"2024-04-02T09:57:21.067314Z","shell.execute_reply":"2024-04-02T09:57:23.465253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Summary**:\n* **Low Incidence Faults**: Stains and Dirtiness are relatively rare.\n* **Moderate Faults**: Z_Scratch and Pastry faults occur more frequently.\n* **Significant Faults**: K_Scatch and Bumps are more common.\n* **Most Common Fault**: Other Faults category is the most prevalent.","metadata":{}},{"cell_type":"code","source":"# Defining categorical and continous features\ncategorical = ['Outside_Global_Index', 'TypeOfSteel_A400' , 'TypeOfSteel_A300']\ncontinous = df.drop(categorical+target, axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T09:57:23.468859Z","iopub.execute_input":"2024-04-02T09:57:23.469975Z","iopub.status.idle":"2024-04-02T09:57:23.478827Z","shell.execute_reply.started":"2024-04-02T09:57:23.469925Z","shell.execute_reply":"2024-04-02T09:57:23.477247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizing Categorical Data Distribution and Target Association\nfor tar in target:\n    print('\\n',tar, '\\n')\n    \n    for cat in categorical:\n        fig, axs = plt.subplots(1, 2, figsize=(15, 4), gridspec_kw={'width_ratios': [1.2, 1.8]})\n\n        # Pie Chart        \n        ax1 = axs[0]\n        subject = df[cat].value_counts().reset_index(name='Count')\n        ax1.pie(subject['Count'], labels=subject[cat], autopct='%1.1f%%', radius=1.2, startangle=30, wedgeprops=dict(width=0.3, edgecolor='w'))   \n        ax1.set_title(f'Share of {cat}')\n\n        # Bar Chart\n        ax2 = axs[1]\n        subject = df.groupby([cat, tar]).size().reset_index(name='Count')\n        pivot_df = subject.pivot(index=cat, columns=tar, values='Count')\n        pivot_df.plot(kind='bar', stacked=True, ax=ax2)\n        ax2.legend([f'{tar}= 0', f'{tar}= 1'], loc=\"upper right\")\n        ax2.set_title(f'{tar} by {cat}')\n        ax2.set_xlabel(cat)\n        ax2.set_ylabel('Count')\n        ax2.set_xticklabels(ax2.get_xticklabels(), rotation=0)\n        ax2.grid(True)\n\n        plt.tight_layout()\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T09:57:23.480594Z","iopub.execute_input":"2024-04-02T09:57:23.481092Z","iopub.status.idle":"2024-04-02T09:57:34.989124Z","shell.execute_reply.started":"2024-04-02T09:57:23.481050Z","shell.execute_reply":"2024-04-02T09:57:34.987744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Histogram Analysis of Continuous Variables by Target Class\nfor tar in target:\n    print('\\n', tar, '\\n')\n    for con in continous:\n        fig, ax = plt.subplots(figsize=(15, 4))\n        sns.histplot(data=df, x=con, hue=tar, bins=50, kde=True)\n        ax.set_title(f'{con} Count by {tar}')\n        ax.grid(True)\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T09:57:34.991138Z","iopub.execute_input":"2024-04-02T09:57:34.991681Z","iopub.status.idle":"2024-04-02T09:59:56.376671Z","shell.execute_reply.started":"2024-04-02T09:57:34.991635Z","shell.execute_reply":"2024-04-02T09:59:56.375585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pre-Processing\n---","metadata":{}},{"cell_type":"markdown","source":"## Data Scaling","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\ndf[features] = scaler.fit_transform(df[features])","metadata":{"execution":{"iopub.status.busy":"2024-04-02T09:59:56.378123Z","iopub.execute_input":"2024-04-02T09:59:56.378486Z","iopub.status.idle":"2024-04-02T09:59:56.489185Z","shell.execute_reply.started":"2024-04-02T09:59:56.378456Z","shell.execute_reply":"2024-04-02T09:59:56.488091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\ndef feature_engineering(df):\n    epsilon = 1e-6\n    \n    # Calculate area\n    df['X_Distance'] = df['X_Maximum'] - df['X_Minimum']\n    df['Y_Distance'] = df['Y_Maximum'] - df['Y_Minimum']\n    df['Area'] = (df['X_Distance']) * (df['Y_Distance'])\n    \n    # Density Feature\n    df['Density'] = df['Pixels_Areas'] / (df['X_Perimeter'] + df['Y_Perimeter'])\n\n    # Calculate perimeter\n    df['Perimeter'] = 2 * ((df['X_Maximum'] - df['X_Minimum']) + (df['Y_Maximum'] - df['Y_Minimum']))\n    \n    # Relative Perimeter Feature\n    df['Relative_Perimeter'] = df['X_Perimeter'] / (df['X_Perimeter'] + df['Y_Perimeter'] + epsilon)\n    \n    # Circularity Feature\n    df['Circularity'] = df['Pixels_Areas'] / (df['X_Perimeter'] ** 2)\n    \n    # Combined Geometric Index Feature\n    df['Combined_Geometric_Index'] = df['Edges_Index'] * df['Square_Index']\n    \n    # Symmetry Index Feature\n    df['Symmetry_Index'] = np.abs(df['X_Distance'] - df['Y_Distance']) / (df['X_Distance'] + df['Y_Distance'] + epsilon)\n    \n    # Compute mean, median, and standard deviation\n    df['Mean_Luminosity'] = df[['Sum_of_Luminosity', 'Minimum_of_Luminosity', 'Maximum_of_Luminosity']].mean(axis=1)\n    df['Median_Luminosity'] = df[['Sum_of_Luminosity', 'Minimum_of_Luminosity', 'Maximum_of_Luminosity']].median(axis=1)\n    df['Std_Luminosity'] = df[['Sum_of_Luminosity', 'Minimum_of_Luminosity', 'Maximum_of_Luminosity']].std(axis=1)\n    \n    # Calculate aspect ratio\n    df['Aspect_Ratio'] = (df['Y_Maximum'] - df['Y_Minimum']) / (df['X_Maximum'] - df['X_Minimum'])\n    \n    # Apply logarithmic transformation\n    df['Log_Pixels_Areas'] = np.log(df['Pixels_Areas'])\n    \n    # Interaction Term Feature\n    df['X_Distance*Pixels_Areas'] = df['X_Distance'] * df['Pixels_Areas']\n    \n    # Create composite feature\n    df['Luminosity_Index_Product'] = df['Luminosity_Index'] * df['Sum_of_Luminosity']\n    \n    # Color Contrast Feature\n    df['Color_Contrast'] = df['Maximum_of_Luminosity'] - df['Minimum_of_Luminosity']\n    \n    # Average Luminosity Feature\n    df['Average_Luminosity'] = (df['Sum_of_Luminosity'] + df['Minimum_of_Luminosity']) / 2\n    \n    # Generate interaction feature\n    df['Area_Pixels_Interact'] = df['Area'] * df['Pixels_Areas']\n    \n    # Additional Features\n    df['sin_orientation'] = np.sin(df['Orientation_Index'])\n    df['Edges_Index2'] = np.exp(df['Edges_Index'] + epsilon)\n    df['X_Maximum2'] = np.sin(df['X_Maximum'])\n    df['Y_Minimum2'] = np.sin(df['Y_Minimum'])\n    df['Aspect_Ratio_Pixels'] = np.where(df['Y_Perimeter'] == 0, 0, df['X_Perimeter'] / df['Y_Perimeter'])\n    df['Aspect_Ratio'] = np.where(df['Y_Distance'] == 0, 0, df['X_Distance'] / df['Y_Distance'])\n\n    # Normalized Steel Thickness Feature\n    df['Normalized_Steel_Thickness'] = (df['Steel_Plate_Thickness'] - df['Steel_Plate_Thickness'].min()) / (df['Steel_Plate_Thickness'].max() - df['Steel_Plate_Thickness'].min())\n\n    # Logarithmic Features\n    df['Log_Perimeter'] = np.log(df['X_Perimeter'] + df['Y_Perimeter'] + epsilon)\n    df['Log_Luminosity'] = np.log(df['Sum_of_Luminosity'] + epsilon)\n    df['Log_Aspect_Ratio'] = np.log(df['Aspect_Ratio'] ** 2 + epsilon)\n\n    # Statistical Features\n    df['Combined_Index'] = df['Orientation_Index'] * df['Luminosity_Index']\n    df['Sigmoid_Areas'] = 1 / (1 + np.exp(-df['LogOfAreas'] + epsilon))\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2024-04-02T09:59:56.493051Z","iopub.execute_input":"2024-04-02T09:59:56.493424Z","iopub.status.idle":"2024-04-02T09:59:56.516338Z","shell.execute_reply.started":"2024-04-02T09:59:56.493393Z","shell.execute_reply":"2024-04-02T09:59:56.515003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Applying feature engineering to the dataframe\ndf = feature_engineering(df)\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-02T09:59:56.517899Z","iopub.execute_input":"2024-04-02T09:59:56.518385Z","iopub.status.idle":"2024-04-02T09:59:56.597215Z","shell.execute_reply.started":"2024-04-02T09:59:56.518354Z","shell.execute_reply":"2024-04-02T09:59:56.595977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking Correlation Matrix with engineered features\nplt.figure(figsize=(15,4))\n\nplt.imshow(df.corr()[target].drop(target, axis=0).T, vmin=-1, vmax=1)\nplt.xticks(range(len(df.corr()[target].drop(target, axis=0).T.columns)), df.corr()[target].drop(target, axis=0).T.columns, rotation=45, ha='right')\nplt.yticks(range(len(target)), target)\nplt.colorbar()\nplt.grid(True)\nplt.title('Correlation Matrix')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T09:59:56.599498Z","iopub.execute_input":"2024-04-02T09:59:56.600806Z","iopub.status.idle":"2024-04-02T09:59:58.238282Z","shell.execute_reply.started":"2024-04-02T09:59:56.600757Z","shell.execute_reply":"2024-04-02T09:59:58.237093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Selection","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import RFECV\nfrom xgboost import XGBClassifier\n\n# Create a XGBoost Classifier\nmodel = XGBClassifier()\n\nX = df[features]\ny = df[target]\n\n# Create an RFECV selector\nrfecv = RFECV(model, step=1, cv=5, scoring='roc_auc')  # Use 5-fold cross-validation\n\n# Fit the RFECV selector\nrfecv.fit(X, y)\n\n# Get the selected feature names\nfeatures = [X.columns[i] for i in rfecv.get_support(indices=True)]\nprint(f\"Selected features ({len(features)}) :\", features)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T09:59:58.239653Z","iopub.execute_input":"2024-04-02T09:59:58.240011Z","iopub.status.idle":"2024-04-02T10:04:28.615142Z","shell.execute_reply.started":"2024-04-02T09:59:58.239980Z","shell.execute_reply":"2024-04-02T10:04:28.614203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Keeping only selected and target features\neng_features = features + target\n\ndf = df[eng_features]\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-02T10:04:28.616877Z","iopub.execute_input":"2024-04-02T10:04:28.617630Z","iopub.status.idle":"2024-04-02T10:04:28.629966Z","shell.execute_reply.started":"2024-04-02T10:04:28.617593Z","shell.execute_reply":"2024-04-02T10:04:28.628786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Label balancing","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 7, figsize=(15, 4))  # 1 row, 7 columns\n\n# Plot pie charts for each column in 'target'\nfor i, colname in enumerate(target):\n    axes[i].pie(df[colname].value_counts(), labels=df[colname].unique(), autopct='%1.1f%%')\n    axes[i].set_title(colname)\n\n# Display the plot\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T10:04:28.631545Z","iopub.execute_input":"2024-04-02T10:04:28.632028Z","iopub.status.idle":"2024-04-02T10:04:29.473698Z","shell.execute_reply.started":"2024-04-02T10:04:28.631982Z","shell.execute_reply":"2024-04-02T10:04:29.472629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Splitting shuffeled data into training and test splits (80/20)\ndf_train, df_test = train_test_split(df, test_size=0.2, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T10:04:29.475121Z","iopub.execute_input":"2024-04-02T10:04:29.475498Z","iopub.status.idle":"2024-04-02T10:04:29.492496Z","shell.execute_reply.started":"2024-04-02T10:04:29.475466Z","shell.execute_reply":"2024-04-02T10:04:29.491128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Applying SMOTE over-sampling technique\nfrom imblearn.over_sampling import SMOTE\n\ndef label_balancing(df):\n    \n    # Creating dataframes for each label\n    Pastry = pd.DataFrame()\n    Z_Scratch = pd.DataFrame()\n    K_Scatch = pd.DataFrame()\n    Stains = pd.DataFrame()\n    Dirtiness = pd.DataFrame()\n    Bumps = pd.DataFrame()\n    Other_Faults = pd.DataFrame()\n    \n    # Initialising SMOTE model\n    sm = SMOTE()\n    \n    # Resampling each label\n    Pastry[df.drop(target, axis=1).columns], Pastry['Pastry'] = sm.fit_resample(df.drop(target, axis=1), df['Pastry'])\n    Z_Scratch[df.drop(target, axis=1).columns], Z_Scratch['Z_Scratch'] = sm.fit_resample(df.drop(target, axis=1), df['Z_Scratch'])\n    K_Scatch[df.drop(target, axis=1).columns], K_Scatch['K_Scatch'] = sm.fit_resample(df.drop(target, axis=1), df['K_Scatch'])\n    Stains[df.drop(target, axis=1).columns], Stains['Stains'] = sm.fit_resample(df.drop(target, axis=1), df['Stains'])\n    Dirtiness[df.drop(target, axis=1).columns], Dirtiness['Dirtiness'] = sm.fit_resample(df.drop(target, axis=1), df['Dirtiness'])\n    Bumps[df.drop(target, axis=1).columns], Bumps['Bumps'] = sm.fit_resample(df.drop(target, axis=1), df['Bumps'])\n    Other_Faults[df.drop(target, axis=1).columns], Other_Faults['Other_Faults'] = sm.fit_resample(df.drop(target, axis=1), df['Other_Faults'])\n\n    return Stains, Dirtiness, Z_Scratch, K_Scatch, Pastry, Bumps, Other_Faults","metadata":{"execution":{"iopub.status.busy":"2024-04-02T10:04:29.497099Z","iopub.execute_input":"2024-04-02T10:04:29.497513Z","iopub.status.idle":"2024-04-02T10:04:29.772490Z","shell.execute_reply.started":"2024-04-02T10:04:29.497480Z","shell.execute_reply":"2024-04-02T10:04:29.771163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Applying label balancing function to each target feature\nStains, Dirtiness, Z_Scratch, K_Scatch, Pastry, Bumps, Other_Faults = label_balancing(df_train)\n\n# Creating a list of dataframes\ntarget_dfs = [Stains, Dirtiness, Z_Scratch, K_Scatch, Pastry, Bumps, Other_Faults]","metadata":{"execution":{"iopub.status.busy":"2024-04-02T10:04:29.775550Z","iopub.execute_input":"2024-04-02T10:04:29.776298Z","iopub.status.idle":"2024-04-02T10:04:30.319308Z","shell.execute_reply.started":"2024-04-02T10:04:29.776252Z","shell.execute_reply":"2024-04-02T10:04:30.318009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualising pie chart of each label after balancing\nfig, axes = plt.subplots(1, 7, figsize=(15, 4))  # 1 row, 7 columns\n\nfor (i, dfname), name in zip(enumerate(target_dfs), target):\n    axes[i].pie(dfname[name].value_counts(), labels=dfname[name].unique(), autopct='%1.1f%%', startangle=45)\n    axes[i].set_title(f'{name} \\n {dfname.shape}')\n\n# Display the plot\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T10:04:30.320775Z","iopub.execute_input":"2024-04-02T10:04:30.321124Z","iopub.status.idle":"2024-04-02T10:04:31.341713Z","shell.execute_reply.started":"2024-04-02T10:04:30.321096Z","shell.execute_reply":"2024-04-02T10:04:31.340526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Training\n---","metadata":{}},{"cell_type":"code","source":"# Defining function that visualises each trained model performance for comparison\ndef compare_models(model_scores):\n    fig, ax = plt.subplots(figsize=(12, 4))\n    bars = ax.bar(model_scores.keys(), model_scores.values())\n    ax.bar_label(bars, label_type=\"edge\")\n    plt.ylim(0, 100)\n    plt.grid(True)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T10:04:31.343470Z","iopub.execute_input":"2024-04-02T10:04:31.344213Z","iopub.status.idle":"2024-04-02T10:04:31.353257Z","shell.execute_reply.started":"2024-04-02T10:04:31.344170Z","shell.execute_reply":"2024-04-02T10:04:31.351285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score\n\n# Initialising models\nclassifiers = {\n    \"Random Forest Classifier\": {'model':RandomForestClassifier()},\n    \n    \"GradientBoostingClassifier\": {'model':GradientBoostingClassifier()},\n    \n    \"XGBoost Classifier\": {'model':XGBClassifier(objective='binary:logistic')}\n    }\n\nbest_models = {}\n\n# Iterating through dataframes of each target label\nfor i, j in zip(target_dfs, target):\n    \n    print('\\n', j, '\\n')\n    \n    # Setting up data\n    X_train = i[features]\n    y_train = i[j]\n    \n    X_test = df_test[features]\n    y_test = df_test[j]\n    \n    model_scores = {}\n    \n    # Iterating through models\n    for key, classifier in classifiers.items():\n        print('Training', key)\n        \n        # Fitting the model\n        try:\n            classifier['model'].fit(X_train, y_train, \n                    early_stopping_rounds=250,\n                    eval_metric='auc',\n                    eval_set=[(X_test, y_test)])\n        except TypeError:\n            classifier['model'''].fit(X_train, y_train)\n            \n        # Evaluating model performance\n        pred = classifier['model'].predict_proba(X_test)\n        pred = [proba[1] for proba in pred]\n        pred = np.array(pred)\n        training_score = roc_auc_score(y_test, pred)\n        model_scores[key] = round(training_score.mean() * 100, 2)\n        print(key, \"has a training score of\", round(training_score.mean() * 100, 2), \"% accuracy score \\n\")\n    \n    # Saving best performing model for current label\n    best_models[j] = [item[0] for item in sorted(model_scores.items(), key=lambda item: item[1], reverse=True)[:1]]\n    \n    # Comparing model performance for current label\n    compare_models(model_scores)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T10:04:31.355426Z","iopub.execute_input":"2024-04-02T10:04:31.356375Z","iopub.status.idle":"2024-04-02T10:07:39.866016Z","shell.execute_reply.started":"2024-04-02T10:04:31.356305Z","shell.execute_reply":"2024-04-02T10:07:39.864473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Optimization\n---","metadata":{}},{"cell_type":"code","source":"# Defining choose model function that sets up model for training\ndef choose_model(params):\n    if best_models[j][0] == \"Random Forest Classifier\":\n        model = RandomForestClassifier(**params)\n    elif best_models[j][0] == \"GradientBoostingClassifier\":\n        model = GradientBoostingClassifier(**params)\n    else:\n        model = XGBClassifier(objective='binary:logistic', **params)\n        \n    return model","metadata":{"execution":{"iopub.status.busy":"2024-04-02T10:09:26.602338Z","iopub.execute_input":"2024-04-02T10:09:26.602755Z","iopub.status.idle":"2024-04-02T10:09:26.609575Z","shell.execute_reply.started":"2024-04-02T10:09:26.602724Z","shell.execute_reply":"2024-04-02T10:09:26.608350Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining objective function for Optuna optimization\ndef objective(trial):\n    \n    # Specifying hyperparameters\n\n    classifiers = {\n        \"Random Forest Classifier\": {\n            'model': RandomForestClassifier(),\n            'params': {\n                'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n                'max_depth': trial.suggest_int('max_depth', 10, 100),\n                'max_features': trial.suggest_categorical('max_features', [1, 'sqrt', 'log2', None]),\n                'n_estimators': trial.suggest_int('n_estimators', 100, 2000)\n            }\n        },\n        \"GradientBoostingClassifier\": {\n            'model': GradientBoostingClassifier(),\n            'params': {\n                'n_estimators': trial.suggest_int('n_estimators', 100, 2000),\n                'learning_rate': trial.suggest_float('learning_rate', 0.01, 1),\n                'max_depth': trial.suggest_int('max_depth', 1, 20),\n                'subsample': trial.suggest_float('subsample', 0.1, 1),\n            }\n        },\n        \"XGBoost Classifier\": {\n            'model': XGBClassifier(objective='binary:logistic'),\n            'params': {\n                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.5),\n                'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n                'gamma': trial.suggest_float('gamma', 0, 1),\n                'subsample': trial.suggest_float('subsample', 0.1, 1),\n                'max_depth': trial.suggest_int('max_depth', 1, 20),\n                'n_estimators': trial.suggest_int('n_estimators', 100, 2000),\n                \"booster\": \"gbtree\",\n                \"reg_alpha\": trial.suggest_float('reg_alpha', 0.1, 1),\n                \"reg_lambda\": trial.suggest_float('reg_lambda', 0, 1),\n                \"colsample_bytree\": trial.suggest_float('colsample_bytree', 0.1, 1)\n            }\n        }\n    }\n    \n    # Selecting best performing model for current label\n    model = choose_model(classifiers[best_models[j][0]]['params'])\n\n    model.fit(X_train, y_train) # Fitting training data to the model\n    \n    pred = model.predict_proba(X_test)\n    pred = [proba[1] for proba in pred]\n    pred = np.array(pred)\n    score = roc_auc_score(y_test, pred) # Evaluating model perfomance using ROC-AUC score\n    \n    return score","metadata":{"execution":{"iopub.status.busy":"2024-04-02T10:36:17.648851Z","iopub.execute_input":"2024-04-02T10:36:17.649295Z","iopub.status.idle":"2024-04-02T10:36:17.664780Z","shell.execute_reply.started":"2024-04-02T10:36:17.649258Z","shell.execute_reply":"2024-04-02T10:36:17.663601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import optuna\n\nmodel_grid_scores = {}\nbest_grid_models = {}\n\n# Iterating through dataframes of each label\nfor i, j in zip(target_dfs, target):\n    \n    print('\\n', j, '\\n')\n\n    # Setting up data\n    X_train = i[features]\n    y_train = i[j]\n    \n    X_test = df_test[features]\n    y_test = df_test[j]\n    \n    # Initializing Optuna study\n    study = optuna.create_study(direction='maximize')\n    \n    # Performing Hyperparameter optimization using optuna objective function\n    print('Training', best_models[j][0], '\\n')\n    study.optimize(objective, n_trials=10) # Number of trials set to 10\n\n    print('Best trial parameters:', study.best_trial.params)\n    print('\\n Best ROC-AUC score:', study.best_trial.value)\n    \n    # Selecting best hyperparameter combination\n    best_trial = study.best_trial # Getting best trial\n    best_params = best_trial.params # Getting best trial parameters\n    parameters = set(classifiers[best_models[j][0]]['model'].get_params().keys()) & set(best_params.keys()) # Getting model parameter keys\n    best_params = {key: best_params[key] for key in parameters} # Choosing parameters appropriate for selected model\n    best = choose_model(best_params) # Choosing the model and assigning its parameters\n    best.fit(X_train, y_train) # Fitting the model\n    \n    best_grid_models[j] = best # Saving the model in a dictionary\n    \n    # Evaluating model performance\n    pred = best.predict_proba(X_test)\n    pred = [proba[1] for proba in pred]\n    pred = np.array(pred)\n    score = roc_auc_score(y_test, pred)\n    model_grid_scores[j] = round(score.mean(), 2) * 100\n    \n    optuna.visualization.plot_optimization_history(study).show() # Visualising Optimization history\n    optuna.visualization.plot_param_importances(study).show() # Visualising Parameter importances\n\n# Evaluating model performances for each label\ncompare_models(model_grid_scores)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T10:37:10.403815Z","iopub.execute_input":"2024-04-02T10:37:10.404279Z","iopub.status.idle":"2024-04-02T10:41:51.534146Z","shell.execute_reply.started":"2024-04-02T10:37:10.404243Z","shell.execute_reply":"2024-04-02T10:41:51.531778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Evaluation\n---","metadata":{}},{"cell_type":"code","source":"# Performing Cross validation using ROC-AUC score for each label\nfrom sklearn.model_selection import cross_val_predict\n\n# Using test data for validation\nX = df_test[features]\n\n# Iterating through dataframe of each label\nfor i, j in zip(target_dfs, target):\n    \n    print('\\n', j, '\\n')\n    \n    y = df_test[j]\n    \n    pred = cross_val_predict(best_grid_models[j], X, y, cv=5, verbose=2)\n    print('Cross validation ROC-AUC score:' ,roc_auc_score(y, pred))","metadata":{"execution":{"iopub.status.busy":"2024-03-25T11:12:15.814278Z","iopub.execute_input":"2024-03-25T11:12:15.815225Z","iopub.status.idle":"2024-03-25T11:14:10.424021Z","shell.execute_reply.started":"2024-03-25T11:12:15.815177Z","shell.execute_reply":"2024-03-25T11:14:10.422762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Performing Classification report for each label\nfrom sklearn.metrics import classification_report\n\n# Using test data for validation\nX = df_test[features]\n\n# Iterating through dataframe of each label\nfor i, j in zip(target_dfs, target):\n    \n    print('\\n', j, '\\n')\n    \n    y = df_test[j]\n    \n    pred = best_grid_models[j].predict(X)\n    print(classification_report(y, pred), '\\n')","metadata":{"execution":{"iopub.status.busy":"2024-03-25T11:16:05.436437Z","iopub.execute_input":"2024-03-25T11:16:05.436924Z","iopub.status.idle":"2024-03-25T11:16:05.911927Z","shell.execute_reply.started":"2024-03-25T11:16:05.436889Z","shell.execute_reply":"2024-03-25T11:16:05.910726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission\n---","metadata":{}},{"cell_type":"code","source":"# Importing test data\npath = r'/kaggle/input/playground-series-s4e3/test.csv'\ndf_test = pd.read_csv(path)\ndf_test.set_index('id', inplace=True)\nid = df_test.index\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-25T11:16:11.587150Z","iopub.execute_input":"2024-03-25T11:16:11.587607Z","iopub.status.idle":"2024-03-25T11:16:11.676853Z","shell.execute_reply.started":"2024-03-25T11:16:11.587571Z","shell.execute_reply":"2024-03-25T11:16:11.674337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Applying feature engineering\ndf_test = feature_engineering(df_test)","metadata":{"execution":{"iopub.status.busy":"2024-03-25T11:16:14.835532Z","iopub.execute_input":"2024-03-25T11:16:14.835981Z","iopub.status.idle":"2024-03-25T11:16:14.878194Z","shell.execute_reply.started":"2024-03-25T11:16:14.835949Z","shell.execute_reply":"2024-03-25T11:16:14.877123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Keeping selected features\ndf_test = df_test[features]","metadata":{"execution":{"iopub.status.busy":"2024-03-25T11:16:15.909328Z","iopub.execute_input":"2024-03-25T11:16:15.909773Z","iopub.status.idle":"2024-03-25T11:16:15.917266Z","shell.execute_reply.started":"2024-03-25T11:16:15.909732Z","shell.execute_reply":"2024-03-25T11:16:15.915609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scaling data\ndf_test = scaler.fit_transform(df_test)","metadata":{"execution":{"iopub.status.busy":"2024-03-25T11:16:16.873474Z","iopub.execute_input":"2024-03-25T11:16:16.874756Z","iopub.status.idle":"2024-03-25T11:16:16.886697Z","shell.execute_reply.started":"2024-03-25T11:16:16.874673Z","shell.execute_reply":"2024-03-25T11:16:16.885161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating submission dataframe with the index same as test data\nsubmission = pd.DataFrame(index=id)\n\n# Creating new dataframes for each label with label balancing using all data available\nStains, Dirtiness, Z_Scratch, K_Scatch, Pastry, Bumps, Other_Faults = label_balancing(df)\n\n# Putting all dataframes into a list\ntarget_dfs = [Stains, Dirtiness, Z_Scratch, K_Scatch, Pastry, Bumps, Other_Faults]\n\n# Iterating through dataframes of each label\nfor i, j in zip(target_dfs, target):\n    \n    print('\\n', j, '\\n')\n    \n    X = i[features]\n    y = i[j]\n    \n    # Training model on full training data\n    model = best_grid_models[j]\n    model.fit(X, y)\n    \n    # Predicting probability\n    results = model.predict_proba(df_test)\n    results = [proba[1] for proba in results]\n    results = np.array(results)\n    submission[j] = results.T","metadata":{"execution":{"iopub.status.busy":"2024-03-25T11:22:20.397144Z","iopub.execute_input":"2024-03-25T11:22:20.397670Z","iopub.status.idle":"2024-03-25T11:26:32.058008Z","shell.execute_reply.started":"2024-03-25T11:22:20.397634Z","shell.execute_reply":"2024-03-25T11:26:32.056840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-25T11:26:51.358874Z","iopub.execute_input":"2024-03-25T11:26:51.359358Z","iopub.status.idle":"2024-03-25T11:26:51.376134Z","shell.execute_reply.started":"2024-03-25T11:26:51.359326Z","shell.execute_reply":"2024-03-25T11:26:51.374761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv')","metadata":{"execution":{"iopub.status.busy":"2024-03-25T11:26:52.299841Z","iopub.execute_input":"2024-03-25T11:26:52.300312Z","iopub.status.idle":"2024-03-25T11:26:52.440833Z","shell.execute_reply.started":"2024-03-25T11:26:52.300278Z","shell.execute_reply":"2024-03-25T11:26:52.439479Z"},"trusted":true},"execution_count":null,"outputs":[]}]}